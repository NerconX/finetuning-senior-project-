# finetuning-senior-project-

<!DOCTYPE html>
<html lang="en">
<head>
   <meta charset="UTF-8">
   <meta name="viewport" content="width=device-width, initial-scale=1.0">
   <title>Pretrained Language Models and Finetuning Approaches</title>
</head>
<body>
   <h1>Pretrained Language Models and Finetuning Approaches</h1>

   <h2>Overview</h2>
   <p>Senior research project exploring the implementation and optimization of pretrained language models (PLMs) through various finetuning techniques. This project demonstrates practical applications of state-of-the-art language models in specific NLP tasks.</p>

   <h2>Models Implemented</h2>
   <ul>
       <li>Google Flan-T5 (780M parameters)</li>
       <li>TII-Falcon 7B</li>
       <li>Facebook OPT-6.7B</li>
   </ul>

   <h2>Tasks and Applications</h2>
   <ol>
       <li>
           <h3>Sentiment Analysis Model</h3>
           <ul>
               <li>Achieved validation loss of 0.026388 after 3 epochs</li>
               <li>Implemented efficient text classification</li>
           </ul>
       </li>
       <li>
           <h3>General Knowledge Chatbot</h3>
           <ul>
               <li>Developed conversational AI capabilities</li>
               <li>Enhanced response generation</li>
           </ul>
       </li>
       <li>
           <h3>Medical Text Inference</h3>
           <ul>
               <li>Improved response relevance by 40%</li>
               <li>Specialized domain adaptation</li>
           </ul>
       </li>
       <li>
           <h3>English Quotes Generation</h3>
           <ul>
               <li>Text generation and style transfer</li>
               <li>Creative content generation</li>
           </ul>
       </li>
   </ol>

   <h2>Key Achievements</h2>
   <ul>
       <li>Reduced model size by 65% while maintaining performance using LoRA and QLoRA</li>
       <li>Improved task-specific accuracy by 15% compared to baseline models</li>
       <li>Conducted extensive testing with 50+ sample inputs</li>
       <li>Implemented comprehensive performance analysis using epoch graphs, training loss, and validation loss metrics</li>
   </ul>

   <h2>Technologies Used</h2>
   <ul>
       <li>Python Programming</li>
       <li>TensorFlow/PyTorch</li>
       <li>Git Version Control</li>
       <li>Cloud Computing Platforms</li>
       <li>Model Evaluation Metrics Tools</li>
       <li>Data Visualization Libraries</li>
   </ul>

   <h2>Key Skills Applied</h2>
   <ul>
       <li>Natural Language Processing (NLP)</li>
       <li>Machine Learning / Deep Learning</li>
       <li>Model Fine-tuning Techniques</li>
       <li>Data Analysis and Visualization</li>
       <li>Model Performance Optimization</li>
       <li>Hyperparameter Tuning</li>
       <li>Research Methodology</li>
   </ul>

   <h2>Project Structure</h2>
   <pre>
project/
├── models/
│   ├── sentiment_analysis/
│   ├── chatbot/
│   ├── medical_text/
│   └── english_quotes/
├── data/
│   └── processed/
├── results/
│   └── metrics/
└── docs/
   └── report/
   </pre>

   <h2>Installation & Requirements</h2>
   <p>[To be added: Installation instructions and dependencies]</p>

   <h2>Usage</h2>
   <p>[To be added: Instructions for running the models and examples]</p>

   <h2>Results and Visualizations</h2>
   <p>[To be added: Performance metrics and visualization examples]</p>

   <h2>Author</h2>
   <p>George Medina<br>
   University of Houston-Downtown<br>
   Fall 2023</p>

   <h2>License</h2>
   <p>[Add your chosen license]</p>

   <h2>Acknowledgments</h2>
   <ul>
       <li>Faculty Advisor: Subash Pakhrin</li>
       <li>Course Instructor: Dr. Ling Xu</li>
   </ul>

</body>
</html>
